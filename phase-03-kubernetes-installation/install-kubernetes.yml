---
# =============================================================================
# Kubernetes Cluster Installation Playbook
# Phase 3: Install production-grade Kubernetes cluster using kubeadm
# 
# CKA Exam Domain: Cluster Architecture, Installation & Configuration (25%)
# =============================================================================
#
# This playbook installs Kubernetes following CKA exam standards:
# - kubeadm for cluster bootstrapping
# - containerd as container runtime
# - Calico CNI for pod networking
#
# Architecture:
# - 1x Control Plane node (k8s-cp-01)
# - 2x Worker nodes (k8s-worker-01, k8s-worker-02)
#
# Usage:
#   ansible-playbook -i inventory.ini install-kubernetes.yml
#
# =============================================================================

- name: "Pre-flight Checks and System Preparation (All Nodes)"
  hosts: k8s_cluster
  become: yes
  tags:
    - preflight
    - all
  tasks:

    # =========================================================================
    # CKA EXAM KNOWLEDGE: Swap must be disabled for kubelet to work
    # Why? Kubelet requires deterministic memory allocation for pods
    # =========================================================================
    - name: Disable swap immediately
      command: swapoff -a
      when: ansible_swaptotal_mb > 0
      changed_when: false

    - name: Disable swap permanently (remove from /etc/fstab)
      lineinfile:
        path: /etc/fstab
        regexp: '.*swap.*'
        state: absent

    - name: Verify swap is disabled
      shell: swapon --show
      register: swap_status
      changed_when: false
      failed_when: swap_status.stdout != ""

    # =========================================================================
    # CKA EXAM KNOWLEDGE: Kernel modules required for container networking
    # - overlay: OverlayFS for container image layers
    # - br_netfilter: Bridge netfilter for iptables rules on bridges
    # =========================================================================
    - name: Load kernel modules for containerd
      modprobe:
        name: "{{ item }}"
        state: present
      loop: "{{ kernel_modules }}"

    - name: Ensure kernel modules load on boot
      copy:
        content: |
          # Kernel modules required for Kubernetes
          overlay
          br_netfilter
        dest: /etc/modules-load.d/k8s.conf
        mode: '0644'

    # =========================================================================
    # CKA EXAM KNOWLEDGE: Sysctl parameters for Kubernetes networking
    # - net.bridge.bridge-nf-call-iptables: Allow iptables to see bridged traffic
    # - net.ipv4.ip_forward: Enable IP forwarding for pod-to-pod communication
    # =========================================================================
    - name: Configure sysctl parameters for Kubernetes
      sysctl:
        name: "{{ item.key }}"
        value: "{{ item.value }}"
        state: present
        reload: yes
        sysctl_file: /etc/sysctl.d/k8s.conf
      loop: "{{ sysctl_config | dict2items }}"

    - name: Update apt cache
      apt:
        update_cache: yes
        cache_valid_time: 3600


# =============================================================================
- name: "Install and Configure containerd (All Nodes)"
  hosts: k8s_cluster
  become: yes
  tags:
    - containerd
    - all
  tasks:

    # =========================================================================
    # CKA EXAM KNOWLEDGE: Why containerd?
    # - Docker deprecated in Kubernetes 1.20+
    # - containerd is the industry standard CRI (Container Runtime Interface)
    # - Used by all major cloud providers (EKS, GKE, AKS)
    # =========================================================================
    - name: Install containerd dependencies
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gnupg
          - lsb-release
        state: present

    - name: Add Docker GPG key (containerd is from Docker repo)
      apt_key:
        url: https://download.docker.com/linux/ubuntu/gpg
        state: present
        keyring: /etc/apt/keyrings/docker.gpg

    - name: Add Docker repository (for containerd)
      apt_repository:
        repo: "deb [arch=amd64 signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} stable"
        state: present
        filename: docker

    - name: Install containerd
      apt:
        name: containerd.io
        state: present
        update_cache: yes

    # =========================================================================
    # CKA EXAM KNOWLEDGE: containerd configuration
    # Default config uses cgroup driver 'cgroupfs', but kubelet expects 'systemd'
    # We must configure containerd to use systemd cgroup driver
    # =========================================================================
    - name: Create containerd config directory
      file:
        path: "{{ containerd_config_dir }}"
        state: directory
        mode: '0755'

    - name: Generate default containerd configuration
      shell: containerd config default > /etc/containerd/config.toml
      args:
        creates: /etc/containerd/config.toml

    - name: Configure containerd to use systemd cgroup driver
      lineinfile:
        path: /etc/containerd/config.toml
        regexp: '^\s+SystemdCgroup = false'
        line: '            SystemdCgroup = true'
        state: present
      notify: Restart containerd

    - name: Enable and start containerd service
      systemd:
        name: containerd
        enabled: yes
        state: started

    - name: Verify containerd is running
      command: systemctl is-active containerd
      register: containerd_status
      changed_when: false
      failed_when: containerd_status.stdout != "active"

  handlers:
    - name: Restart containerd
      systemd:
        name: containerd
        state: restarted


# =============================================================================
- name: "Install Kubernetes Components (All Nodes)"
  hosts: k8s_cluster
  become: yes
  tags:
    - kubernetes-packages
    - all
  tasks:

    # =========================================================================
    # CKA EXAM KNOWLEDGE: Three core Kubernetes packages
    # - kubeadm: Tool to bootstrap the cluster
    # - kubelet: Node agent that runs on every node
    # - kubectl: CLI tool to interact with cluster (optional on workers)
    # =========================================================================
    - name: Install required packages for Kubernetes repository
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gpg
        state: present

    - name: Create keyrings directory
      file:
        path: /etc/apt/keyrings
        state: directory
        mode: '0755'

    - name: Add Kubernetes GPG key
      shell: |
        curl -fsSL https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/Release.key | \
        gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
      args:
        creates: /etc/apt/keyrings/kubernetes-apt-keyring.gpg

    - name: Add Kubernetes repository
      apt_repository:
        repo: "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v{{ kubernetes_version }}/deb/ /"
        state: present
        filename: kubernetes

    - name: Install Kubernetes packages
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present
        update_cache: yes

    # =========================================================================
    # CKA EXAM KNOWLEDGE: Why hold packages?
    # Prevents accidental upgrades via 'apt upgrade'
    # Kubernetes upgrades must be controlled (control plane first, then workers)
    # =========================================================================
    - name: Hold Kubernetes packages (prevent automatic upgrades)
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl

    - name: Enable kubelet service (will start after kubeadm init/join)
      systemd:
        name: kubelet
        enabled: yes


# =============================================================================
- name: "Initialize Kubernetes Control Plane"
  hosts: control_plane
  become: yes
  tags:
    - control-plane
    - all
  tasks:

    # =========================================================================
    # CKA EXAM KNOWLEDGE: Check if cluster already initialized
    # Prevents re-running 'kubeadm init' which would fail
    # /etc/kubernetes/admin.conf only exists after successful init
    # =========================================================================
    - name: Check if Kubernetes is already initialized
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeadm_init_stat

    # =========================================================================
    # CKA EXAM KNOWLEDGE: kubeadm init breakdown
    # 
    # What happens during 'kubeadm init':
    # 1. Preflight checks (swap, ports, etc.)
    # 2. Generate CA certificates → /etc/kubernetes/pki/
    # 3. Generate kubeconfig files → /etc/kubernetes/*.conf
    # 4. Generate static pod manifests → /etc/kubernetes/manifests/
    #    - kube-apiserver.yaml
    #    - kube-controller-manager.yaml
    #    - kube-scheduler.yaml
    #    - etcd.yaml
    # 5. Wait for control plane pods to start
    # 6. Create bootstrap token (for worker nodes to join)
    # 7. Install CoreDNS and kube-proxy addons
    #
    # Flags explained:
    # --pod-network-cidr: CIDR for pod IPs (must match CNI plugin)
    # --service-cidr: CIDR for service IPs
    # --apiserver-advertise-address: IP for API server (control plane node IP)
    # =========================================================================
    - name: Initialize Kubernetes control plane with kubeadm
      command: >
        kubeadm init
        --pod-network-cidr={{ pod_network_cidr }}
        --service-cidr={{ service_cidr }}
        --apiserver-advertise-address={{ apiserver_advertise_address }}
        {{ kubeadm_init_extra_args }}
      register: kubeadm_init_output
      when: not kubeadm_init_stat.stat.exists

    - name: Display kubeadm init output (SAVE THE JOIN COMMAND!)
      debug:
        var: kubeadm_init_output.stdout_lines
      when: kubeadm_init_output is defined and kubeadm_init_output.stdout_lines is defined

    # =========================================================================
    # CKA EXAM KNOWLEDGE: kubeconfig files
    # - admin.conf: Full cluster admin access (use this for kubectl)
    # - kubelet.conf: Kubelet uses this to authenticate to API server
    # - controller-manager.conf: Controller manager authentication
    # - scheduler.conf: Scheduler authentication
    # =========================================================================
    - name: Create .kube directory for non-root user
      file:
        path: "{{ kubernetes_user_home }}/.kube"
        state: directory
        owner: "{{ kubernetes_user }}"
        group: "{{ kubernetes_user }}"
        mode: '0755'

    - name: Copy admin.conf to user's kubeconfig
      copy:
        src: /etc/kubernetes/admin.conf
        dest: "{{ kubernetes_user_home }}/.kube/config"
        owner: "{{ kubernetes_user }}"
        group: "{{ kubernetes_user }}"
        mode: '0600'
        remote_src: yes

    # =========================================================================
    # CKA EXAM KNOWLEDGE: Bootstrap token management
    # Tokens expire after 24 hours by default
    # Use 'kubeadm token create --print-join-command' to generate new token
    # =========================================================================
    - name: Generate kubeadm join command for workers
      command: kubeadm token create --print-join-command
      register: kubeadm_join_command
      changed_when: false

    - name: Save join command to local file
      local_action:
        module: copy
        content: "{{ kubeadm_join_command.stdout }}"
        dest: /tmp/kubeadm-join-command.sh
      become: no

    - name: Display join command
      debug:
        msg: "Worker nodes will use: {{ kubeadm_join_command.stdout }}"


# =============================================================================
- name: "Install Calico CNI Plugin"
  hosts: control_plane
  become: yes
  become_user: "{{ kubernetes_user }}"
  tags:
    - cni
    - all
  tasks:

    # =========================================================================
    # CKA EXAM KNOWLEDGE: Why do we need CNI?
    # - CNI (Container Network Interface) enables pod-to-pod communication
    # - Without CNI, pods cannot get IP addresses
    # - Calico provides pod networking + NetworkPolicy enforcement
    #
    # Why install CNI after kubeadm init?
    # - Control plane pods (API server, etc.) use host networking
    # - They don't need CNI to start
    # - But CoreDNS pods need CNI to get IPs (they'll be Pending without it)
    # =========================================================================
    - name: Check if Calico is already installed
      command: kubectl get pods -n kube-system -l k8s-app=calico-node
      register: calico_check
      changed_when: false
      failed_when: false

    - name: Download Calico manifest
      get_url:
        url: "{{ calico_manifest_url }}"
        dest: /tmp/calico.yaml
        mode: '0644'
      when: calico_check.rc != 0

    # =========================================================================
    # CKA EXAM KNOWLEDGE: Calico CIDR configuration
    # Calico default CIDR is 192.168.0.0/16
    # We're using 10.244.0.0/16, so we must modify the manifest
    # Look for CALICO_IPV4POOL_CIDR in the manifest
    # =========================================================================
    - name: Configure Calico to use correct pod CIDR
      replace:
        path: /tmp/calico.yaml
        regexp: '192.168.0.0/16'
        replace: "{{ pod_network_cidr }}"
      when: calico_check.rc != 0

    - name: Apply Calico manifest
      command: kubectl apply -f /tmp/calico.yaml
      when: calico_check.rc != 0

    - name: Wait for Calico pods to be ready (this may take 2-3 minutes)
      command: kubectl wait --for=condition=ready pod -l k8s-app=calico-node -n kube-system --timeout=300s
      register: calico_ready
      retries: 3
      delay: 10
      until: calico_ready.rc == 0

    - name: Verify Calico installation
      command: kubectl get pods -n kube-system -l k8s-app=calico-node
      register: calico_pods
      changed_when: false

    - name: Display Calico pods status
      debug:
        var: calico_pods.stdout_lines


# =============================================================================
- name: "Join Worker Nodes to Cluster"
  hosts: workers
  become: yes
  tags:
    - workers
    - all
  tasks:

    # =========================================================================
    # CKA EXAM KNOWLEDGE: Check if node already joined
    # /etc/kubernetes/kubelet.conf only exists after successful join
    # =========================================================================
    - name: Check if node is already part of cluster
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf_stat

    - name: Copy join command from control plane
      copy:
        src: /tmp/kubeadm-join-command.sh
        dest: /tmp/kubeadm-join-command.sh
        mode: '0700'
      when: not kubelet_conf_stat.stat.exists

    # =========================================================================
    # CKA EXAM KNOWLEDGE: kubeadm join breakdown
    #
    # What happens during 'kubeadm join':
    # 1. Connect to API server using bootstrap token
    # 2. Download cluster CA certificate
    # 3. Verify CA using discovery-token-ca-cert-hash
    # 4. Generate kubelet client certificate
    # 5. Create kubelet.conf
    # 6. Start kubelet service
    # 7. Kubelet registers node with cluster
    #
    # The join command format:
    # kubeadm join <control-plane-ip>:6443 \
    #   --token <token> \
    #   --discovery-token-ca-cert-hash sha256:<hash>
    # =========================================================================
    - name: Join worker node to cluster
      command: bash /tmp/kubeadm-join-command.sh
      register: kubeadm_join_output
      when: not kubelet_conf_stat.stat.exists

    - name: Display join output
      debug:
        var: kubeadm_join_output.stdout_lines
      when: kubeadm_join_output is defined and kubeadm_join_output.stdout_lines is defined

    - name: Verify kubelet is running
      systemd:
        name: kubelet
        state: started
        enabled: yes


# =============================================================================
- name: "Verify Cluster Installation"
  hosts: control_plane
  become: yes
  become_user: "{{ kubernetes_user }}"
  tags:
    - verify
    - all
  tasks:

    - name: Wait for all nodes to be Ready (may take 1-2 minutes)
      command: kubectl wait --for=condition=ready nodes --all --timeout=300s
      register: nodes_ready
      retries: 3
      delay: 10
      until: nodes_ready.rc == 0

    - name: Get cluster nodes
      command: kubectl get nodes -o wide
      register: cluster_nodes
      changed_when: false

    - name: Display cluster nodes
      debug:
        var: cluster_nodes.stdout_lines

    - name: Get all pods in kube-system namespace
      command: kubectl get pods -n kube-system -o wide
      register: system_pods
      changed_when: false

    - name: Display system pods
      debug:
        var: system_pods.stdout_lines

    - name: Get cluster info
      command: kubectl cluster-info
      register: cluster_info
      changed_when: false

    - name: Display cluster info
      debug:
        var: cluster_info.stdout_lines

    - name: Verify all system pods are running
      shell: kubectl get pods -n kube-system --field-selector=status.phase!=Running,status.phase!=Succeeded
      register: non_running_pods
      changed_when: false
      failed_when: non_running_pods.stdout != ""

    # =========================================================================
    # CKA EXAM KNOWLEDGE: Component health checks
    # - Control plane components run as static pods
    # - Check /etc/kubernetes/manifests/ to see pod definitions
    # - Use 'kubectl logs' to debug component issues
    # =========================================================================
    - name: Final health check summary
      debug:
        msg:
          - "=========================================="
          - "   Kubernetes Cluster Installation Complete!"
          - "=========================================="
          - ""
          - "Cluster endpoint: https://{{ apiserver_advertise_address }}:6443"
          - "Pod network CIDR: {{ pod_network_cidr }}"
          - "Service CIDR: {{ service_cidr }}"
          - "CNI plugin: {{ cni_plugin }}"
          - ""
          - "To access cluster from your local machine:"
          - "  scp {{ kubernetes_user }}@{{ apiserver_advertise_address }}:~/.kube/config ~/.kube/config"
          - ""
          - "Next steps:"
          - "  1. kubectl get nodes"
          - "  2. kubectl get pods -A"
          - "  3. Deploy test application"
          - ""
          - "=========================================="
